{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17340,"status":"ok","timestamp":1701729160749,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"},"user_tz":300},"id":"mPgJKsiQOtru","outputId":"b7a93007-69a5-4867-c1e8-d62261296401"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"markdown","metadata":{"id":"5pP_sIdITZNO"},"source":["## Document information\n","* The bbc.docs file will provide information about all the articles.\n","* The bbc.terms file will contain the list of terms.\n","* Use the bbc.mtx file to obtain term frequencies for each article.\n","\n","\n","* For example, in row 812: (2, 528, 5.0) indicates that term 2, \"sale,\" occurs 5 times in article 528 (entertainment.018).\n","    * If you check the terms file, the second term is \"sale.\" Checking the docs file reveals that the 528th article is \"entertainment.018.\""]},{"cell_type":"markdown","metadata":{"id":"AYwoWrUNPHZy"},"source":["# Finding the input X\n","\n","* The input X will be a matrix where each row represents an article, and each column represents a term.\n","\n","\n","* Each element (i, j) in the matrix will indicate the frequency or binary presence of the j-th term within the i-th article.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":168,"status":"ok","timestamp":1701729163995,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"},"user_tz":300},"id":"9DVqyT68OwqL"},"outputs":[],"source":["def process_input_data():\n","  file_path = \"filepath/bbc/bbc.mtx\"\n","\n","  with open(file_path) as f:\n","      lines = [line.rstrip() for line in f]\n","\n","  lines_without_first_two_rows = lines[2:]\n","\n","  split_values = [line.split() for line in lines_without_first_two_rows]\n","\n","  split_values = [[int(term_id), int(article_id), float(frequency)] for term_id, article_id, frequency in split_values]\n","\n","  # 9635 terms and 2225 articles\n","  unique_terms = set(term_id for term_id, _, _ in split_values)\n","  unique_articles = set(article_id for _, article_id, _ in split_values)\n","\n","  article_term_matrix = np.zeros((len(unique_articles), len(unique_terms)))\n","\n","  df_matrix = pd.DataFrame(article_term_matrix, index=sorted(unique_articles), columns=sorted(unique_terms))\n","\n","  for term_id, article_id, frequency in split_values:\n","      df_matrix.at[article_id, term_id] = frequency\n","\n","  X = np.array(df_matrix)\n","\n","  return X"]},{"cell_type":"markdown","metadata":{"id":"tFMGRg2qPSQL"},"source":["# Finding labels Y\n","* Y will represent the corresponding label for each row in the matrix.\n","\n","\n","* The labels are encoded into numeric values as follows: Business (0), Entertainment (1), Politics (2), Sports (3), Tech (4)."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":138,"status":"ok","timestamp":1701729167428,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"},"user_tz":300},"id":"6hcRmK_VPfGO"},"outputs":[],"source":["def process_input_labels():\n","  file_path_labels = \"/content/drive/My Drive/CSCC11/A2/bbc/bbc.docs\"\n","  with open(file_path_labels) as f_labels:\n","      article_categories = [line.strip().split('.')[0] for line in f_labels]\n","\n","  Y = np.array(article_categories)\n","\n","  label_encoder = LabelEncoder()\n","  numeric_Y = label_encoder.fit_transform(Y)\n","\n","  return numeric_Y"]},{"cell_type":"markdown","metadata":{"id":"Cp7hWEhXQbVg"},"source":["# Partition Data"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":127,"status":"ok","timestamp":1701729170748,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"},"user_tz":300},"id":"LlQdM0b4OeYi"},"outputs":[],"source":["def process_data(isNB):\n","\n","  X = process_input_data()\n","  Y = process_input_labels()\n","\n","  if(isNB):\n","    X = (X > 0).astype(int)\n","\n","  X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3, random_state=42)\n","\n","  return (X_train, X_test, Y_train, Y_test)\n"]},{"cell_type":"markdown","metadata":{"id":"Q4u2ZEsHy6Aj"},"source":["# Part a)"]},{"cell_type":"markdown","metadata":{"id":"LTo-V7q5iOuG"},"source":["# Calculate Prior\n","* The prior probabilities represent the probability of each category.\n","\n","\n","* For example, the probability of the category being \"Business\" is calculated as the ratio of the number of business articles to the total number of articles:\n"," \n"," \n","    $\\frac{\\text{the number of business articles}}{\\text{the number of articles}}$\n","\n","# Calculate Conditionals\n","* We consider binary values in the case of Naive Bayes, where an article either contains the term (1) or does not (0).\n","\n","\n","* We aim to determine how frequently a term appears in an article category.\n","\n","\n","* For example: \n","\n","    $\\frac{\\text{the number of business articles that contain the term \"sale\"}}{\\text{the number of business articles}}$\n","\n","\n","* In cases where the conditional probability is 0, we generalize by adding 1 to the numerator and 2 to the denominator.\n","## Code Explanation\n","* We create a dictionary where the five categories serve as keys, and the values consist of arrays containing the conditional probabilities of each term.\n","\n","\n","* This is achieved by utilizing a boolean mask, where the mask is set to True when the label Y is equal to the current category and False otherwise.\n","\n","\n","* Essentially, the mask selects the rows in the binary term frequency matrix that correspond to the articles in the current category.\n","\n","\n","* We employ this approach to enable the subsequent line to calculate the sum of binary values for each term across all articles in the current category. This allows us to utilize the formula demonstrated earlier in the example above."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":191,"status":"ok","timestamp":1701729178520,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"},"user_tz":300},"id":"-dv8hOE2jEGv"},"outputs":[],"source":["def calculate_probabilities(X_train, Y_train):\n","\n","  #Priors\n","\n","  unique_categories, category_counts = np.unique(Y_train, return_counts=True)\n","  prior_probs = category_counts / len(Y_train)\n","\n","  # Priors probability check\n","  # for category, prior_prob in zip(unique_categories, prior_probs):\n","  #   print(f\"Category: {category}, Prior Probability: {prior_prob:.4f}\")\n","\n","  #Frequencies\n","\n","  term_probs_by_category = {c: np.zeros(X_train.shape[1]) for c in unique_categories}\n","\n","  for i, category in enumerate(unique_categories):\n","      category_mask = (Y_train == category)\n","      term_probs_by_category[category] = (X_train[category_mask].sum(axis=0) + 1) / (category_counts[i] + 2)\n","\n","  return prior_probs, term_probs_by_category\n"]},{"cell_type":"markdown","metadata":{"id":"Jj4wS0yyjrTd"},"source":["# Prediction"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":111,"status":"ok","timestamp":1701729180881,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"},"user_tz":300},"id":"sspBTVd_jquN"},"outputs":[],"source":["def predict_naive_bayes(X_test, prior_probs, term_probs_by_category):\n","    predictions = []\n","\n","    for i in range(X_test.shape[0]):\n","        article_probs = []\n","\n","        for idx, category in enumerate(term_probs_by_category):\n","            prior_prob = np.log(prior_probs[idx])\n","\n","            # We take the conditional probability\n","            # If the term is present in the article (boolean mask 1) we take that conditional probability\n","            # We do the similar thing if the term is not present in article and the probability of that happening\n","            conditional_prob = np.sum(np.log(term_probs_by_category[category][X_test[i] == 1])) + \\\n","                               np.sum(np.log(1 - term_probs_by_category[category][X_test[i] == 0]))\n","\n","            article_prob = prior_prob + conditional_prob\n","            article_probs.append(article_prob)\n","\n","        predicted_category = np.argmax(article_probs)\n","        predictions.append(predicted_category)\n","\n","    return predictions"]},{"cell_type":"markdown","metadata":{"id":"mZC9hA76p05q"},"source":["# Training Accuracy"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":111,"status":"ok","timestamp":1701729182889,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"},"user_tz":300},"id":"ray17XYgp38t"},"outputs":[],"source":["def calculate_accuracy(actual_labels, predicted_labels):\n","    correct_predictions = np.sum(actual_labels == predicted_labels)\n","    total_predictions = len(actual_labels)\n","    accuracy = correct_predictions / total_predictions * 100\n","    return accuracy"]},{"cell_type":"markdown","metadata":{"id":"ZAoUH6ZKj46k"},"source":["# Trial and Training Accuracy"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12224,"status":"ok","timestamp":1701729196661,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"},"user_tz":300},"id":"X2JbHz6Tj6wl","outputId":"5707271a-340a-4487-bfd8-b113a9cc7f04"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Accuracy: 98.84%\n","Testing Accuracy: 95.96%\n"]}],"source":["X_train, X_test, Y_train, Y_test = process_data(True)\n","\n","prior_probs, term_probs_by_category = calculate_probabilities(X_train, Y_train)\n","\n","actual_train_labels = Y_train\n","actual_test_labels = Y_test\n","\n","predicted_train_labels = predict_naive_bayes(X_train, prior_probs, term_probs_by_category)\n","predicted_test_labels = predict_naive_bayes(X_test, prior_probs, term_probs_by_category)\n","\n","train_accuracy = calculate_accuracy(actual_train_labels, predicted_train_labels)\n","print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n","\n","test_accuracy = calculate_accuracy(actual_test_labels, predicted_test_labels)\n","print(f\"Testing Accuracy: {test_accuracy:.2f}%\")\n"]},{"cell_type":"markdown","metadata":{"id":"GJCVDBwbzXnk"},"source":["# Part b) Gaussian Naive Bayes"]},{"cell_type":"markdown","metadata":{"id":"cMmkNunpoZ_O"},"source":["# Finding the mean and variance for each\n","* In Gaussian Naive Bayes, our objective is to determine the mean and variance for each term in every category.\n","\n","\n","* This is essential in establishing the Gaussian Distribution associated with each term for each category.\n","\n","\n","* To accomplish this, we create a dictionary for each category that contains a matrix of articles by terms, where each element (i, j) represents the associated frequency.\n","\n","\n","* This matrix structure facilitates the subsequent calculation of mean and variance.\n","\n","\n","* Note: If the variance is 0, we set it to \\(e^{-9}\\) as suggested."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":218,"status":"ok","timestamp":1701729212714,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"},"user_tz":300},"id":"lfaknHZmoc3m"},"outputs":[],"source":["def calculate_mean_variance_by_category(X_train, Y_train):\n","    unique_categories, _ = np.unique(Y_train, return_counts=True)\n","\n","    term_stats_by_category = {c: {'mean': np.zeros(X_train.shape[1]), 'variance': np.zeros(X_train.shape[1])} for c in unique_categories}\n","\n","    for category in unique_categories:\n","        category_mask = (Y_train == category)\n","        term_stats_by_category[category]['mean'] = np.mean(X_train[category_mask], axis=0)\n","        term_stats_by_category[category]['variance'] = np.var(X_train[category_mask], axis=0)\n","\n","        term_stats_by_category[category]['variance'][term_stats_by_category[category]['variance'] == 0] = 1e-9\n","\n","    return term_stats_by_category"]},{"cell_type":"markdown","metadata":{"id":"wJw-7EdDpza-"},"source":["# Gaussian Classifier\n","* With the means and variances obtained from the previous function, we can now proceed to classify new data.\n","\n","\n","* The classification process involves multiplying the prior probabilities by the conditional probabilities obtained from the Gaussian distributions.\n","\n","\n","* To address potential underflow issues, we take the logarithm, and the classification involves summing the logs.\n","\n","\n","* This calculation is performed for each category, and ultimately, the article with the highest value becomes our classification for that article."]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1701729215519,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"},"user_tz":300},"id":"VGImpBN9p17H"},"outputs":[],"source":["def predict_gaussian(X_test, prior_probs, term_stats_by_category):\n","    predictions = []\n","\n","    for i in range(X_test.shape[0]):\n","        category_probs = []\n","\n","        for category, stats in term_stats_by_category.items():\n","            prior_prob = np.log(prior_probs[category])\n","\n","            log_likelihoods = -(np.log(2 * np.pi * stats['variance']) +\n","                                      ((X_test[i] - stats['mean'])**2) / (2 * stats['variance']))\n","\n","            likelihood_sum = np.sum(log_likelihoods)\n","            category_prob = prior_prob + likelihood_sum\n","            category_probs.append(category_prob)\n","\n","        predicted_category = list(term_stats_by_category.keys())[np.argmax(category_probs)]\n","        predictions.append(predicted_category)\n","\n","    return predictions"]},{"cell_type":"markdown","metadata":{"id":"GlZMvv9xwO2R"},"source":["# Trial and Training Accuracy"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8928,"status":"ok","timestamp":1701729228971,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"},"user_tz":300},"id":"-5zghgLIwR6T","outputId":"07066f7a-da05-460d-c449-3bc592768ca6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Accuracy: 100.00%\n","Testing Accuracy: 92.07%\n"]}],"source":["X_train, X_test, Y_train, Y_test = process_data(False)\n","\n","\n","prior_probs, term_probs_by_category = calculate_probabilities(X_train, Y_train)\n","term_stats_by_category = calculate_mean_variance_by_category(X_train, Y_train)\n","\n","actual_train_labels = Y_train\n","actual_test_labels = Y_test\n","\n","predicted_train_labels = predict_gaussian(X_train, prior_probs, term_stats_by_category)\n","predicted_test_labels = predict_gaussian(X_test, prior_probs, term_stats_by_category)\n","\n","train_accuracy = calculate_accuracy(actual_train_labels, predicted_train_labels)\n","print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n","\n","test_accuracy = calculate_accuracy(actual_test_labels, predicted_test_labels)\n","print(f\"Testing Accuracy: {test_accuracy:.2f}%\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNvdQmEH+wn17XnA3TGRdu4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
