{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNvdQmEH+wn17XnA3TGRdu4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"mPgJKsiQOtru","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701729160749,"user_tz":300,"elapsed":17340,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"}},"outputId":"b7a93007-69a5-4867-c1e8-d62261296401"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"markdown","source":["## Document information\n","* The bbc.docs file will give us the information about all the articles\n","* The bbc.terms file will get the list of terms\n","* The bbc.mtx file to obtain the term frequencies for each article\n","*Example would be row 812(2, 528, 5.0) which indicates term 2 \"sale occures 5 times in article 528 (entertainment.018).\n","* If you check the terms file the second term is sale, and if you check the docs file the 528th article is entertainment.018."],"metadata":{"id":"5pP_sIdITZNO"}},{"cell_type":"markdown","source":["# Finding the input X\n","\n","* The input X will be a matrix where the rows represent each article and the columns will represent each term.\n","* Each i,j in the matrix will be the frequency/binary of the term within the article\n"],"metadata":{"id":"AYwoWrUNPHZy"}},{"cell_type":"code","source":["def process_input_data():\n","  file_path = \"/content/drive/My Drive/CSCC11/A2/bbc/bbc.mtx\"\n","  df = pd.read_csv(file_path)\n","\n","  with open(file_path) as f:\n","      lines = [line.rstrip() for line in f]\n","\n","  lines_without_first_two_rows = lines[2:]\n","\n","  split_values = [line.split() for line in lines_without_first_two_rows]\n","\n","  split_values = [[int(term_id), int(article_id), float(frequency)] for term_id, article_id, frequency in split_values]\n","\n","  # 9635 terms and 2225 articles\n","  unique_terms = set(term_id for term_id, _, _ in split_values)\n","  unique_articles = set(article_id for _, article_id, _ in split_values)\n","\n","  article_term_matrix = np.zeros((len(unique_articles), len(unique_terms)))\n","\n","  df_matrix = pd.DataFrame(article_term_matrix, index=sorted(unique_articles), columns=sorted(unique_terms))\n","\n","  for term_id, article_id, frequency in split_values:\n","      df_matrix.at[article_id, term_id] = frequency\n","\n","  X = np.array(df_matrix)\n","\n","  return X"],"metadata":{"id":"9DVqyT68OwqL","executionInfo":{"status":"ok","timestamp":1701729163995,"user_tz":300,"elapsed":168,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Finding labels Y\n","* Y will be the corresponding label to each row in the matrix\n","* We encode the labels to numeric values\n","* Business: 0, entertainment: 1, politics: 2, sports: 3, tech: 4"],"metadata":{"id":"tFMGRg2qPSQL"}},{"cell_type":"code","source":["def process_input_labels():\n","  file_path_labels = \"/content/drive/My Drive/CSCC11/A2/bbc/bbc.docs\"\n","  with open(file_path_labels) as f_labels:\n","      article_categories = [line.strip().split('.')[0] for line in f_labels]\n","\n","  Y = np.array(article_categories)\n","\n","  label_encoder = LabelEncoder()\n","  numeric_Y = label_encoder.fit_transform(Y)\n","\n","  return numeric_Y"],"metadata":{"id":"6hcRmK_VPfGO","executionInfo":{"status":"ok","timestamp":1701729167428,"user_tz":300,"elapsed":138,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Partition Data"],"metadata":{"id":"Cp7hWEhXQbVg"}},{"cell_type":"code","source":["def process_data(isNB):\n","\n","  X = process_input_data()\n","  Y = process_input_labels()\n","\n","  if(isNB):\n","    X = (X > 0).astype(int)\n","\n","  X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3, random_state=42)\n","\n","  return (X_train, X_test, Y_train, Y_test)\n"],"metadata":{"id":"LlQdM0b4OeYi","executionInfo":{"status":"ok","timestamp":1701729170748,"user_tz":300,"elapsed":127,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Part a)"],"metadata":{"id":"Q4u2ZEsHy6Aj"}},{"cell_type":"markdown","source":["# Calculate Prior\n","* The prior probabilities is the probability of the category.\n","* For example, the probability of a category being business will be: $\\frac{\\text{the number of business articles}}{\\text{the number of articles}}$\n","\n","\n","# Calculate Conditionals\n","* We consider the binary values in the case of Naive Bayes.\n","* I.e if the article contains the term the value be 1 and 0 otherwirse.\n","* We want to check how often a term appears in a article category.\n","* For example: $\\frac{\\text{the number of business articles that contain the term \"sale\"}}{\\text{the number of business articles}}$\n","* In the case that conditional probability is 0, we will generlize and add 1 to the numerator and 2 to the denominator.\n","## Code Explanation\n","* We create a dictionary where the 5 catergories are the keys, and the values will be an array of the conditional probabilities of each term.\n","* We do by using a boolean mask, True where the label Y is equal to the current category and False otherwise.\n","* I.e it essentially selects the rows in the binary term frequency matrix that correspond to the articles in the current category.\n","* We use this so the next line calculates the sum of binary values for each term across all articles in the current category. Allowing us to use the formula showed earlier in the example above"],"metadata":{"id":"LTo-V7q5iOuG"}},{"cell_type":"code","source":["def calculate_probabilities(X_train, Y_train):\n","\n","  #Priors\n","\n","  unique_categories, category_counts = np.unique(Y_train, return_counts=True)\n","  prior_probs = category_counts / len(Y_train)\n","\n","  # Priors probability check\n","  # for category, prior_prob in zip(unique_categories, prior_probs):\n","  #   print(f\"Category: {category}, Prior Probability: {prior_prob:.4f}\")\n","\n","  #Frequencies\n","\n","  term_probs_by_category = {c: np.zeros(X_train.shape[1]) for c in unique_categories}\n","\n","  for i, category in enumerate(unique_categories):\n","      category_mask = (Y_train == category)\n","      term_probs_by_category[category] = (X_train[category_mask].sum(axis=0) + 1) / (category_counts[i] + 2)\n","\n","  return prior_probs, term_probs_by_category\n"],"metadata":{"id":"-dv8hOE2jEGv","executionInfo":{"status":"ok","timestamp":1701729178520,"user_tz":300,"elapsed":191,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Prediction"],"metadata":{"id":"Jj4wS0yyjrTd"}},{"cell_type":"code","source":["def predict_naive_bayes(X_test, prior_probs, term_probs_by_category):\n","    predictions = []\n","\n","    for i in range(X_test.shape[0]):\n","        article_probs = []\n","\n","        for idx, category in enumerate(term_probs_by_category):\n","            prior_prob = np.log(prior_probs[idx])\n","\n","            # We take the conditional probability\n","            # If the term is present in the article (boolean mask 1) we take that conditional probability\n","            # We do the similar thing if the term is not present in article and the probability of that happening\n","            conditional_prob = np.sum(np.log(term_probs_by_category[category][X_test[i] == 1])) + \\\n","                               np.sum(np.log(1 - term_probs_by_category[category][X_test[i] == 0]))\n","\n","            article_prob = prior_prob + conditional_prob\n","            article_probs.append(article_prob)\n","\n","        predicted_category = np.argmax(article_probs)\n","        predictions.append(predicted_category)\n","\n","    return predictions"],"metadata":{"id":"sspBTVd_jquN","executionInfo":{"status":"ok","timestamp":1701729180881,"user_tz":300,"elapsed":111,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Training Accuracy"],"metadata":{"id":"mZC9hA76p05q"}},{"cell_type":"code","source":["def calculate_accuracy(actual_labels, predicted_labels):\n","    correct_predictions = np.sum(actual_labels == predicted_labels)\n","    total_predictions = len(actual_labels)\n","    accuracy = correct_predictions / total_predictions * 100\n","    return accuracy"],"metadata":{"id":"ray17XYgp38t","executionInfo":{"status":"ok","timestamp":1701729182889,"user_tz":300,"elapsed":111,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Trial and Training Accuracy"],"metadata":{"id":"ZAoUH6ZKj46k"}},{"cell_type":"code","source":["X_train, X_test, Y_train, Y_test = process_data(True)\n","\n","prior_probs, term_probs_by_category = calculate_probabilities(X_train, Y_train)\n","\n","actual_train_labels = Y_train\n","actual_test_labels = Y_test\n","\n","predicted_train_labels = predict_naive_bayes(X_train, prior_probs, term_probs_by_category)\n","predicted_test_labels = predict_naive_bayes(X_test, prior_probs, term_probs_by_category)\n","\n","train_accuracy = calculate_accuracy(actual_train_labels, predicted_train_labels)\n","print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n","\n","test_accuracy = calculate_accuracy(actual_test_labels, predicted_test_labels)\n","print(f\"Testing Accuracy: {test_accuracy:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2JbHz6Tj6wl","executionInfo":{"status":"ok","timestamp":1701729196661,"user_tz":300,"elapsed":12224,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"}},"outputId":"5707271a-340a-4487-bfd8-b113a9cc7f04"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy: 98.84%\n","Testing Accuracy: 95.96%\n"]}]},{"cell_type":"markdown","source":["# Part b) Gaussian Naive Bayes"],"metadata":{"id":"GJCVDBwbzXnk"}},{"cell_type":"markdown","source":["# Finding the mean and variance for each\n","* For Gaussian Naive Bayes we want to find the mean and variance for each term in each category.\n","* We do this to help us find the Gaussian Distribution associated with each term for each category.\n","* To do this we create a dictionary for each category containing a matrix of articles by terms where each i,j contains the associated frequency.\n","* This allows us to then calculate the mean and variance more easily.\n","* Note: if the variance is 0, we make it to $e^{-9}$ as suggested."],"metadata":{"id":"cMmkNunpoZ_O"}},{"cell_type":"code","source":["def calculate_mean_variance_by_category(X_train, Y_train):\n","    unique_categories, _ = np.unique(Y_train, return_counts=True)\n","\n","    term_stats_by_category = {c: {'mean': np.zeros(X_train.shape[1]), 'variance': np.zeros(X_train.shape[1])} for c in unique_categories}\n","\n","    for category in unique_categories:\n","        category_mask = (Y_train == category)\n","        term_stats_by_category[category]['mean'] = np.mean(X_train[category_mask], axis=0)\n","        term_stats_by_category[category]['variance'] = np.var(X_train[category_mask], axis=0)\n","\n","        term_stats_by_category[category]['variance'][term_stats_by_category[category]['variance'] == 0] = 1e-9\n","\n","    return term_stats_by_category"],"metadata":{"id":"lfaknHZmoc3m","executionInfo":{"status":"ok","timestamp":1701729212714,"user_tz":300,"elapsed":218,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Gaussian Classifier\n","* Using the means and variances obtained from the previous function we can now classify new data.\n","* To classify data we will use the prior probabilities multipled the conditional probabilities (obtained from the gaussians).\n","* We will also want to take the log to handle underflow, so we will now be summing the logs.\n","* We caculate this for each category and in the end, the article with the highest value will be our classification for the article."],"metadata":{"id":"wJw-7EdDpza-"}},{"cell_type":"code","source":["def predict_gaussian(X_test, prior_probs, term_stats_by_category):\n","    predictions = []\n","\n","    for i in range(X_test.shape[0]):\n","        category_probs = []\n","\n","        for category, stats in term_stats_by_category.items():\n","            prior_prob = np.log(prior_probs[category])\n","\n","            log_likelihoods = -(np.log(2 * np.pi * stats['variance']) +\n","                                      ((X_test[i] - stats['mean'])**2) / (2 * stats['variance']))\n","\n","            likelihood_sum = np.sum(log_likelihoods)\n","            category_prob = prior_prob + likelihood_sum\n","            category_probs.append(category_prob)\n","\n","        predicted_category = list(term_stats_by_category.keys())[np.argmax(category_probs)]\n","        predictions.append(predicted_category)\n","\n","    return predictions"],"metadata":{"id":"VGImpBN9p17H","executionInfo":{"status":"ok","timestamp":1701729215519,"user_tz":300,"elapsed":236,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Trial and Training Accuracy"],"metadata":{"id":"GlZMvv9xwO2R"}},{"cell_type":"code","source":["X_train, X_test, Y_train, Y_test = process_data(False)\n","\n","\n","prior_probs, term_probs_by_category = calculate_probabilities(X_train, Y_train)\n","term_stats_by_category = calculate_mean_variance_by_category(X_train, Y_train)\n","\n","actual_train_labels = Y_train\n","actual_test_labels = Y_test\n","\n","predicted_train_labels = predict_gaussian(X_train, prior_probs, term_stats_by_category)\n","predicted_test_labels = predict_gaussian(X_test, prior_probs, term_stats_by_category)\n","\n","train_accuracy = calculate_accuracy(actual_train_labels, predicted_train_labels)\n","print(f\"Training Accuracy: {train_accuracy:.2f}%\")\n","\n","test_accuracy = calculate_accuracy(actual_test_labels, predicted_test_labels)\n","print(f\"Testing Accuracy: {test_accuracy:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5zghgLIwR6T","executionInfo":{"status":"ok","timestamp":1701729228971,"user_tz":300,"elapsed":8928,"user":{"displayName":"Ashwin Mlk","userId":"13857911411660462110"}},"outputId":"07066f7a-da05-460d-c449-3bc592768ca6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy: 100.00%\n","Testing Accuracy: 92.07%\n"]}]}]}